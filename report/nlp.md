基于chinese-bert-wwm-ext基模型进行迁移学习，从基模型开始训练，学习率为2e-5,batch_size为64，各种指标包括accuracy,precision,recall,f1从0.484左右到0.62左右，后有下降趋势。我们通过分析，认为学习率和batch_size都会影响损失函数的收敛进而影响模型性能，需要动态调整这两个超参数。

| epoch | avg_train_loss | avg_val_loss | accuracy | precision | recall  | f1      |
| ----- | -------------- | ------------ | -------- | --------- | ------- | ------- |
| 1     | 0.88297        | 1.56563      | 0.48396  | 0.58481   | 0.48400 | 0.47886 |
| 2     | 0.37394        | 2.07491      | 0.53740  | 0.58859   | 0.53758 | 0.54097 |
| 3     | 0.13350        | 2.47994      | 0.54978  | 0.62239   | 0.55008 | 0.55502 |
| 4     | 0.12432        | 2.34744      | 0.56799  | 0.60513   | 0.56800 | 0.57767 |
| 5     | 0.11404        | 2.70061      | 0.53981  | 0.63287   | 0.54025 | 0.54436 |
| 6     | 0.08748        | 2.38641      | 0.57613  | 0.61947   | 0.57617 | 0.58524 |
| 7     | 0.07484        | 2.52791      | 0.58735  | 0.61402   | 0.58717 | 0.58956 |
| 8     | 0.07029        | 2.32303      | 0.61312  | 0.61708   | 0.61317 | 0.61397 |
| 9     | 0.05084        | 2.39103      | 0.60721  | 0.61727   | 0.60717 | 0.60818 |
| 10    | 0.04642        | 2.41405      | 0.60032  | 0.61458   | 0.60042 | 0.60075 |
| 11    | 0.03524        | 2.57129      | 0.59899  | 0.61209   | 0.59900 | 0.59883 |
| 12    | 0.03386        | 2.67493      | 0.60489  | 0.61195   | 0.60483 | 0.60642 |
| 13    | 0.03517        | 2.63839      | 0.60173  | 0.62042   | 0.60167 | 0.60265 |
| 14    | 0.03331        | 2.82664      | 0.57289  | 0.61225   | 0.57292 | 0.57293 |
| 15    | 0.02488        | 2.76904      | 0.60464  | 0.61811   | 0.60450 | 0.60374 |
| 16    | 0.02325        | 2.79710      | 0.61860  | 0.62124   | 0.61858 | 0.61619 |
| 17    | 0.02416        | 2.78872      | 0.61503  | 0.61387   | 0.61500 | 0.61196 |
| 18    | 0.02218        | 2.84258      | 0.60040  | 0.60310   | 0.60017 | 0.59765 |
| 19    | 0.01940        | 2.97235      | 0.60655  | 0.60807   | 0.60633 | 0.60499 |
| 20    | 0.01495        | 3.00849      | 0.60414  | 0.60671   | 0.60400 | 0.60221 |
| 21    | 0.01347        | 2.98428      | 0.61037  | 0.60798   | 0.61017 | 0.60779 |
| 22    | 0.01353        | 3.15384      | 0.61345  | 0.61600   | 0.61333 | 0.61074 |
| 23    | 0.01696        | 3.11305      | 0.61070  | 0.61073   | 0.61075 | 0.60758 |
| 24    | 0.01638        | 3.03822      | 0.60397  | 0.60966   | 0.60408 | 0.60325 |
| 25    | 0.01174        | 3.19096      | 0.60322  | 0.61220   | 0.60300 | 0.60047 |
| 26    | 0.01826        | 3.11693      | 0.60979  | 0.60871   | 0.60967 | 0.60741 |
| 27    | 0.00955        | 3.13252      | 0.61752  | 0.61371   | 0.61742 | 0.61376 |
| 28    | 0.01388        | 3.16761      | 0.61802  | 0.62082   | 0.61792 | 0.61324 |
| 29    | 0.01401        | 3.10958      | 0.61179  | 0.61830   | 0.61167 | 0.60899 |
| 30    | 0.00982        | 3.03178      | 0.62866  | 0.62475   | 0.62858 | 0.62490 |
| 31    | 0.01360        | 3.10490      | 0.62284  | 0.62130   | 0.62267 | 0.61978 |
| 32    | 0.00863        | 3.13126      | 0.61669  | 0.61844   | 0.61667 | 0.61434 |
| 33    | 0.00883        | 3.31205      | 0.61561  | 0.62004   | 0.61567 | 0.61366 |
| 34    | 0.00823        | 3.17936      | 0.61295  | 0.61640   | 0.61283 | 0.61196 |
| 35    | 0.00861        | 3.24342      | 0.61312  | 0.61570   | 0.61317 | 0.61087 |
| 36    | 0.00841        | 3.18969      | 0.62400  | 0.62346   | 0.62417 | 0.62098 |
| 37    | 0.00877        | 3.33488      | 0.61918  | 0.61789   | 0.61933 | 0.61590 |
| 38    | 0.00632        | 3.39017      | 0.61702  | 0.61650   | 0.61700 | 0.61278 |
| 39    | 0.00766        | 3.45101      | 0.60871  | 0.61352   | 0.60867 | 0.60813 |
| 40    | 0.00695        | 3.45868      | 0.60896  | 0.60921   | 0.60892 | 0.60697 |
| 41    | 0.00694        | 3.48716      | 0.59483  | 0.60422   | 0.59475 | 0.59446 |
| 42    | 0.00642        | 3.63950      | 0.60090  | 0.61090   | 0.60083 | 0.59777 |
| 43    | 0.00776        | 3.51143      | 0.61436  | 0.61875   | 0.61425 | 0.61071 |
| 44    | 0.00678        | 3.43201      | 0.60065  | 0.61074   | 0.60058 | 0.59978 |
| 45    | 0.00664        | 3.32009      | 0.61370  | 0.61662   | 0.61383 | 0.61367 |
| 46    | 0.00612        | 3.40333      | 0.61046  | 0.61134   | 0.61033 | 0.60778 |
| 47    | 0.00538        | 3.47241      | 0.60846  | 0.61316   | 0.60850 | 0.60858 |
| 48    | 0.00729        | 3.39610      | 0.61735  | 0.61973   | 0.61733 | 0.61346 |
| 49    | 0.00735        | 3.47096      | 0.61386  | 0.62094   | 0.61375 | 0.61176 |
| 50    | 0.00574        | 3.47063      | 0.60987  | 0.61794   | 0.60992 | 0.61169 |
| 51    | 0.00572        | 3.51998      | 0.61262  | 0.61159   | 0.61258 | 0.61052 |
| 52    | 0.00613        | 3.41234      | 0.60713  | 0.60996   | 0.60717 | 0.60601 |
| 53    | 0.00732        | 3.40813      | 0.61428  | 0.61920   | 0.61442 | 0.61344 |
| 54    | 0.00443        | 3.52471      | 0.60472  | 0.61485   | 0.60467 | 0.60420 |
| 55    | 0.00403        | 3.60054      | 0.60938  | 0.61496   | 0.60942 | 0.60679 |
| 56    | 0.00535        | 3.73974      | 0.59026  | 0.60945   | 0.59042 | 0.59095 |
| 57    | 0.00496        | 3.63722      | 0.60721  | 0.61484   | 0.60725 | 0.60717 |
| 58    | 0.00424        | 3.82100      | 0.59957  | 0.61594   | 0.59958 | 0.59937 |
| 59    | 0.00457        | 3.70928      | 0.61361  | 0.61846   | 0.61342 | 0.61064 |
| 60    | 0.00453        | 3.71564      | 0.60987  | 0.61620   | 0.60975 | 0.60870 |
| 61    | 0.00446        | 3.70644      | 0.60805  | 0.61344   | 0.60792 | 0.60494 |
| 62    | 0.00625        | 3.65437      | 0.60630  | 0.60944   | 0.60625 | 0.60464 |
| 63    | 0.00711        | 3.71761      | 0.60655  | 0.61397   | 0.60658 | 0.60562 |
| 64    | 0.00426        | 3.74618      | 0.61112  | 0.61451   | 0.61100 | 0.60778 |
| 65    | 0.00528        | 3.79894      | 0.58810  | 0.60996   | 0.58817 | 0.58848 |
| 66    | 0.00500        | 3.74591      | 0.60647  | 0.61173   | 0.60667 | 0.60443 |
| 67    | 0.00381        | 3.70908      | 0.60514  | 0.61324   | 0.60525 | 0.60452 |
| 68    | 0.00382        | 3.98539      | 0.58910  | 0.61206   | 0.58917 | 0.58904 |
| 69    | 0.00376        | 3.97911      | 0.60040  | 0.61485   | 0.60033 | 0.59808 |
| 70    | 0.00407        | 3.90970      | 0.60223  | 0.61419   | 0.60225 | 0.60175 |
| 71    | 0.00342        | 3.87971      | 0.60555  | 0.61614   | 0.60550 | 0.60378 |
| 72    | 0.00326        | 3.93411      | 0.59824  | 0.61292   | 0.59817 | 0.59792 |
| 73    | 0.00394        | 3.95720      | 0.59574  | 0.61420   | 0.59575 | 0.59566 |
| 74    | 0.00488        | 3.97496      | 0.59441  | 0.61358   | 0.59450 | 0.59396 |
| 75    | 0.00431        | 3.73905      | 0.60530  | 0.60828   | 0.60542 | 0.60525 |
| 76    | 0.00375        | 3.93161      | 0.59242  | 0.61150   | 0.59242 | 0.59205 |
| 77    | 0.00329        | 3.91574      | 0.58968  | 0.60594   | 0.58967 | 0.58867 |
| 78    | 0.00383        | 3.80522      | 0.59915  | 0.60459   | 0.59908 | 0.59749 |
| 79    | 0.00357        | 3.78064      | 0.60613  | 0.61313   | 0.60600 | 0.60604 |
| 80    | 0.00388        | 3.97329      | 0.59225  | 0.60938   | 0.59217 | 0.59016 |
| 81    | 0.00359        | 3.87440      | 0.60231  | 0.60695   | 0.60225 | 0.59959 |
| 82    | 0.00343        | 3.93424      | 0.59591  | 0.61081   | 0.59575 | 0.59624 |
| 83    | 0.00332        | 3.98388      | 0.59092  | 0.61000   | 0.59083 | 0.59144 |
| 84    | 0.00334        | 4.08525      | 0.58386  | 0.60864   | 0.58383 | 0.58355 |
| 85    | 0.00337        | 4.06640      | 0.58793  | 0.60977   | 0.58792 | 0.58751 |
| 86    | 0.00463        | 3.88600      | 0.60239  | 0.60676   | 0.60250 | 0.59968 |
| 87    | 0.00444        | 3.93548      | 0.59417  | 0.61114   | 0.59425 | 0.59431 |
| 88    | 0.00378        | 3.85536      | 0.60065  | 0.60640   | 0.60075 | 0.59974 |
| 89    | 0.00333        | 3.93312      | 0.59683  | 0.61597   | 0.59683 | 0.59807 |
| 90    | 0.00325        | 3.96134      | 0.59707  | 0.61386   | 0.59717 | 0.59728 |

通过训练效果很好的模型model.plt，进一步训练，仅训练5轮，各项指标达到94%。

| epoch | avg_train_loss | avg_val_loss | accuracy | precision | recall  | f1      |
| ----- | -------------- | ------------ | -------- | --------- | ------- | :------ |
| 1     | 0.14675        | 0.36626      | 0.93143  | 0.93211   | 0.93125 | 0.93134 |
| 2     | 0.11257        | 0.36102      | 0.93268  | 0.93374   | 0.93250 | 0.93258 |
| 3     | 0.08687        | 0.35738      | 0.93451  | 0.93521   | 0.93433 | 0.93437 |
| 4     | 0.07157        | 0.34302      | 0.93974  | 0.93987   | 0.93958 | 0.93951 |
| 5     | 0.05883        | 0.34809      | 0.94058  | 0.94082   | 0.94042 | 0.94029 |